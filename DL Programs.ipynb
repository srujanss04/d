{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpaFZKrapwCt"
   },
   "source": [
    "# ***`1`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5oAuqhkNpoty",
    "outputId": "a49ea4df-1101-4229-d99e-be9610b9df47"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hebbian(x, y, w, lr=0.1):\n",
    "    return w + lr * np.outer(x, y)\n",
    "\n",
    "def perceptron(x, y, w, lr=0.1):\n",
    "    return w + lr * x * y\n",
    "\n",
    "def delta(x, y, w, lr=0.1):\n",
    "    return w + lr * (y - np.dot(w, x)) * x\n",
    "\n",
    "def correlation(x, y, w, lr=0.1):\n",
    "    return w + lr * np.outer(x, y)\n",
    "\n",
    "def outstar(x, y, w, lr=0.1):\n",
    "    return w + lr * (y - np.dot(w, x))\n",
    "\n",
    "# Example usage\n",
    "x = np.array([1, -1, 0, 0.5])\n",
    "y = 1\n",
    "w = np.array([0.2, -0.1, 0.0, 0.1])\n",
    "\n",
    "w_hebbian = hebbian(x, y, w)\n",
    "w_perceptron = perceptron(x, y, w)\n",
    "w_delta = delta(x, y, w)\n",
    "w_correlation = correlation(x, y, w)\n",
    "w_outstar = outstar(x, [y], w)\n",
    "\n",
    "print(\"Hebbian:\", w_hebbian)\n",
    "print(\"Perceptron:\", w_perceptron)\n",
    "print(\"Delta:\", w_delta)\n",
    "print(\"Correlation:\", w_correlation)\n",
    "print(\"OutStar:\", w_outstar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOIbHkV5p6zV"
   },
   "source": [
    "# ***`2`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "0oqYcV85p8wF",
    "outputId": "4f8432c2-9908-4f16-9ee2-682649373eb3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "plt.plot(x, 1 / (1 + np.exp(-x)), label='Sigmoid')\n",
    "plt.plot(x, np.tanh(x), label='Tanh')\n",
    "plt.plot(x, np.maximum(0, x), label='ReLU')\n",
    "plt.plot(x, np.where(x > 0, x, x * 0.01), label='Leaky ReLU')\n",
    "plt.plot(x, np.exp(x) / np.sum(np.exp(x)), label='Softmax')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABFS0JR7q8Wt"
   },
   "source": [
    "## ***`3`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9WgtI3ceq-jF",
    "outputId": "1ff07264-ca71-4aab-9e6e-48d6653b9a41"
   },
   "outputs": [],
   "source": [
    "def perceptron(inputs, weights, bias):\n",
    "    activation = sum(i * w for i, w in zip(inputs, weights)) + bias\n",
    "    return 1 if activation >= 0 else 0\n",
    "\n",
    "inputs = [1, 1, 1]\n",
    "weights = [0.2, 0.4, 0.2]\n",
    "bias = -0.5\n",
    "\n",
    "expected_output = 1\n",
    "\n",
    "output = perceptron(inputs, weights, bias)\n",
    "\n",
    "accuracy = 100 if output == expected_output else 0\n",
    "\n",
    "print(\"Output:\", output)\n",
    "print(\"Accuracy:\", accuracy, \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXCJJXj8sWx1"
   },
   "source": [
    "### ***`4`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYAuaPEks5Wd"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def process_image(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    equalized = cv2.equalizeHist(img)\n",
    "    _, thresholded = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY)\n",
    "    edges = cv2.Canny(img, 100, 200)\n",
    "    flipped = cv2.flip(img, 1)\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    morphed = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    cv2.imshow('Original', img)\n",
    "    cv2.imshow('Equalized', equalized)\n",
    "    cv2.imshow('Thresholded', thresholded)\n",
    "    cv2.imshow('Edges', edges)\n",
    "    cv2.imshow('Flipped', flipped)\n",
    "    cv2.imshow('Morphed', morphed)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "process_image('/home/lab705/Downloads/1.jpeg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYj94Ymjtk_-"
   },
   "source": [
    "### ***`5`***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "id": "ptQs5ZJ5tkce",
    "outputId": "a38932a3-24d8-47a7-aeb5-c93ffc1c3c33"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_and_process_image(image_path):\n",
    "    img = tf.image.resize(tf.image.decode_image(tf.io.read_file(image_path), channels=3), [512, 512])\n",
    "    return img[tf.newaxis, ...] / 255.0\n",
    "\n",
    "model = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')\n",
    "\n",
    "content_image = load_and_process_image('/home/lab705/Downloads/1.jpeg')\n",
    "style_image = load_and_process_image('/home/lab705/Downloads/2.jpeg')\n",
    "\n",
    "stylized_image = model(content_image, style_image)[0]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i, img in enumerate([content_image, style_image, stylized_image], 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.imshow(img[0])\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMstJ2IXxRfe"
   },
   "source": [
    "### ***`6`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "esNUGz-5xApu",
    "outputId": "3d8195b6-2d38-49ef-810c-67c8150a6394"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    layers.Flatten(input_shape=(32, 32, 3)),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "num_samples_to_display = 5\n",
    "for i in range(num_samples_to_display):\n",
    "    print(f\"Sample {i+1}: True Label: {y_test[i][0]}, Predicted Label: {predicted_labels[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQjPevsUx8wP"
   },
   "source": [
    "### ***`7`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kuNrEhl0x-m2",
    "outputId": "fe513adf-c739-457e-9821-8e77a64620c6"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)) / 255.0\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)) / 255.0\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=5, validation_data=(test_images, test_labels))\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXOayScOyvom"
   },
   "source": [
    "### ***`8`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xv74GKR8yxQm"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load and prepare data\n",
    "data = pd.read_csv('/home/lab705/DL/sonar.csv')\n",
    "X = StandardScaler().fit_transform(data.drop('Target', axis=1))\n",
    "y = LabelEncoder().fit_transform(data['Target'])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build and compile model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(60, activation='relu', input_dim=60),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(30, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train and evaluate model\n",
    "model.fit(X_train, y_train, epochs=50, validation_split=0.1)\n",
    "print(\"Test Accuracy:\", model.evaluate(X_test, y_test)[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5j2ICh8tyyj2"
   },
   "source": [
    "### ***`9`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2MtV9wtvy0LX",
    "outputId": "18d87126-3e30-413c-887b-3ecb09060e03"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def make_generator():\n",
    "    return Sequential([\n",
    "        layers.Dense(7*7*256, use_bias=False, input_shape=(100,)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Reshape((7, 7, 256)),\n",
    "        layers.Conv2DTranspose(128, 5, strides=1, padding='same', use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Conv2DTranspose(64, 5, strides=2, padding='same', use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Conv2DTranspose(1, 5, strides=2, padding='same', use_bias=False, activation='tanh')\n",
    "    ])\n",
    "\n",
    "def make_discriminator():\n",
    "    return Sequential([\n",
    "        layers.Conv2D(64, 5, strides=2, padding='same', input_shape=(28, 28, 1)),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Conv2D(128, 5, strides=2, padding='same'),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "generator = make_generator()\n",
    "discriminator = make_discriminator()\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "gen_opt = tf.keras.optimizers.Adam(1e-4)\n",
    "disc_opt = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "@tf.function\n",
    "def train_step(images, batch_size=32):\n",
    "    noise = tf.random.normal([batch_size, 100])\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "        gen_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "        disc_loss = cross_entropy(tf.ones_like(real_output), real_output) + cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    generator_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    gen_opt.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
    "    disc_opt.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(train_images, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5  # Normalize to [-1, 1]\n",
    "\n",
    "batch_size = 256\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(60000).batch(batch_size)\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "num_examples_to_generate = 16\n",
    "seed = tf.random.normal([num_examples_to_generate, 100])\n",
    "\n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    predictions = model(test_input, training=False)\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow((predictions[i, :, :, 0] + 1) / 2, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for image_batch in train_dataset:\n",
    "        train_step(image_batch)\n",
    "\n",
    "    generate_and_save_images(generator, epoch + 1, seed)\n",
    "\n",
    "generate_and_save_images(generator, epochs, seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9KSETtVzXPm"
   },
   "source": [
    "### ***`10`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UmPriI1AzZ_9",
    "outputId": "42f53f68-c4b4-4c81-d233-03253dd92382"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "def train_model(use_batch_norm=False):\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(128, activation='relu' if not use_batch_norm else None),\n",
    "        (BatchNormalization() if use_batch_norm else Dropout(0.2)),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=5, verbose=0)\n",
    "    return model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Dropout Model Accuracy:\", train_model(use_batch_norm=False)[1])\n",
    "print(\"Batch Norm Model Accuracy:\", train_model(use_batch_norm=True)[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVSeS9V00Qz_"
   },
   "source": [
    "### ***`11`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s5lFGNNO6PzH",
    "outputId": "3f479bca-619b-4d61-8a30-7e3292d7c472"
   },
   "outputs": [],
   "source": [
    "!pip install tf-models-official\n",
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "URFq5z0k7V3A",
    "outputId": "17c1c711-787d-47ef-ae4f-fbae3526710b"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install opencv-python\n",
    "!pip install numpy==1.19.5  # Install a specific version (replace 1.19.5 with a compatible version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "id": "2gyI_2Pr0S93",
    "outputId": "e81950a5-d47d-4a57-e8ef-c75e00d190b9"
   },
   "outputs": [],
   "source": [
    "# Clone the Mask R-CNN repository if not already present\n",
    "!git clone https://github.com/akTwelve/Mask_RCNN.git\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import skimage.io\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set up paths and configurations\n",
    "ROOT_DIR = \"Mask_RCNN\"\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "IMAGE_PATH = \"/content/abcd.png\"\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "\n",
    "# Import Mask R-CNN and COCO configuration\n",
    "sys.path.append(ROOT_DIR)\n",
    "from mrcnn import utils, model as modellib, visualize\n",
    "sys.path.append(os.path.join(ROOT_DIR, \"samples/coco/\"))\n",
    "import coco\n",
    "\n",
    "# Download COCO weights if not already present\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "# Inference configuration for CPU\n",
    "class InferenceConfig(coco.CocoConfig):\n",
    "    GPU_COUNT = 0\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "config = InferenceConfig()\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True)\n",
    "\n",
    "# Load image\n",
    "image = skimage.io.imread(IMAGE_PATH)\n",
    "\n",
    "# Perform detection\n",
    "results = model.detect([image], verbose=0)\n",
    "r = results[0]\n",
    "\n",
    "# Class names for COCO dataset\n",
    "class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "               'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n",
    "               'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
    "               'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n",
    "               'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n",
    "               'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
    "               'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork',\n",
    "               'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n",
    "               'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
    "               'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv',\n",
    "               'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',\n",
    "               'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "\n",
    "# Visualize results\n",
    "visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xL-DBRhi22h_"
   },
   "source": [
    "### ***`14`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bUNXE7Sl24Mn",
    "outputId": "3dca9b1f-869d-4882-f80c-e3d5888662ed"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# Load IMDB dataset (using top 5000 most frequent words)\n",
    "vocab_size = 5000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "# Pad sequences to have the same length\n",
    "max_len = 500\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "# Build GRU model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 128),\n",
    "    tf.keras.layers.GRU(128),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/aleju/imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U cython "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"git+https://github.com/philferriere/cocoapi.git#egg=pycocotools&subdirectory=PythonAPI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
